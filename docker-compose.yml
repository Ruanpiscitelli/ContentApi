version: '3.8'

# Configurações comuns para todos os serviços
x-common-settings: &common-settings
  restart: unless-stopped
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"
  ulimits:
    memlock: -1
    stack: 67108864
  shm_size: '2gb'
  security_opt:
    - no-new-privileges:true
  read_only: true
  tmpfs:
    - /tmp:exec,mode=777
  cap_drop:
    - ALL
  depends_on:
    redis:
      condition: service_healthy
  init: true
  dns:
    - 8.8.8.8
    - 8.8.4.4
  stop_grace_period: 30s

# Variáveis de ambiente comuns
x-common-env: &common-env
  PYTHONPATH: /app
  CUDA_HOME: /usr/local/cuda
  TORCH_CUDA_ARCH_LIST: "8.6"
  SKIP_MODEL_DOWNLOAD: "true"
  XFORMERS_ENABLE_FLASH_ATTENTION: "true"

# Configuração padrão de healthcheck
x-healthcheck: &default-healthcheck
  test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 60s

# Configuração padrão de recursos
x-resources: &service-resources
  limits:
    cpus: '4'
    memory: 16G
  reservations:
    devices:
      - driver: nvidia
        count: 1
        capabilities: [gpu, utility, compute]

services:
  video-generator:
    <<: *common-settings
    build:
      context: .
      dockerfile: meu-servidor-ia/services/video_generator/Dockerfile
    volumes:
      - ./models:/app/models:ro
      - ./meu-servidor-ia/shared:/app/shared:ro
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: "0"
      CUDA_VISIBLE_DEVICES: "0"
    ports:
      - "8001:8000"
    healthcheck: *default-healthcheck
    deploy:
      resources: *service-resources
    runtime: nvidia

  image-generator:
    <<: *common-settings
    build:
      context: .
      dockerfile: meu-servidor-ia/services/image_generator/Dockerfile
    volumes:
      - ./models:/app/models:ro
      - ./meu-servidor-ia/shared:/app/shared:ro
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: "1"
      CUDA_VISIBLE_DEVICES: "1"
    ports:
      - "8002:8000"
    healthcheck: *default-healthcheck
    deploy:
      resources: *service-resources
    runtime: nvidia

  text-generation:
    <<: *common-settings
    build:
      context: .
      dockerfile: meu-servidor-ia/services/text_generation/Dockerfile
    volumes:
      - ./models:/app/models:ro
      - ./meu-servidor-ia/shared:/app/shared:ro
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: "2"
      CUDA_VISIBLE_DEVICES: "2"
    ports:
      - "8003:8000"
    healthcheck: *default-healthcheck
    deploy:
      resources: *service-resources
    runtime: nvidia

  voice-generator:
    <<: *common-settings
    build:
      context: .
      dockerfile: meu-servidor-ia/services/voice_generator/Dockerfile
    volumes:
      - ./models:/app/models:ro
      - ./meu-servidor-ia/shared:/app/shared:ro
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: "3"
      CUDA_VISIBLE_DEVICES: "3"
    ports:
      - "8004:8000"
    healthcheck: *default-healthcheck
    deploy:
      resources: *service-resources
    runtime: nvidia

  video-editor:
    <<: *common-settings
    build:
      context: .
      dockerfile: meu-servidor-ia/services/video_editor/Dockerfile
    volumes:
      - ./models:/app/models:ro
      - ./meu-servidor-ia/shared:/app/shared:ro
      - ./storage:/app/storage
      - ./logs/editor:/app/logs
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: "3"  # Compartilhando GPU com voice-generator
      CUDA_VISIBLE_DEVICES: "3"
    ports:
      - "8005:8000"
    healthcheck: *default-healthcheck
    deploy:
      resources: *service-resources
    runtime: nvidia

  redis:
    image: redis:7-alpine
    command: >
      --save 60 1
      --loglevel warning
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          memory: 1G
    cap_add:
      - IPC_LOCK

  dashboard:
    <<: *common-settings
    build:
      context: .
      dockerfile: meu-servidor-ia/services/dashboard/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - API_HOST=0.0.0.0
      - API_PORT=8000
    depends_on:
      - video-generator
      - image-generator
      - text-generation
      - voice-generator
      - video-editor
    healthcheck: *default-healthcheck

networks:
  default:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/24

volumes:
  redis_data: 