{
    "default_model": "mistralai/Mistral-Small-24B-Instruct-2501",
    "models": {
        "mistralai/Mistral-Small-24B-Instruct-2501": {
            "type": "vllm",
            "max_tokens": 32768,
            "temperature": 0.15,
            "top_p": 0.95,
            "batch_size": 32,
            "quantization": "awq",
            "gpu_memory_utilization": 0.90,
            "tensor_parallel_size": 1,
            "cache_dir": "models/text/cache/mistral-small-24b",
            "checkpoint_dir": "models/text/checkpoints/mistral-small-24b"
        },
        "meta-llama/Llama-2-7b-chat-hf": {
            "type": "vllm",
            "max_tokens": 4096,
            "temperature": 0.7,
            "top_p": 0.95,
            "batch_size": 32,
            "quantization": "awq",
            "gpu_memory_utilization": 0.90,
            "tensor_parallel_size": 1,
            "cache_dir": "models/text/cache/llama-7b",
            "checkpoint_dir": "models/text/checkpoints/llama-7b"
        },
        "meta-llama/Llama-2-13b-chat-hf": {
            "type": "vllm",
            "max_tokens": 4096,
            "temperature": 0.7,
            "top_p": 0.95,
            "batch_size": 16,
            "quantization": "awq",
            "gpu_memory_utilization": 0.90,
            "tensor_parallel_size": 2,
            "cache_dir": "models/text/cache/llama-13b",
            "checkpoint_dir": "models/text/checkpoints/llama-13b"
        },
        "meta-llama/Llama-2-70b-chat-hf": {
            "type": "vllm",
            "max_tokens": 4096,
            "temperature": 0.7,
            "top_p": 0.95,
            "batch_size": 4,
            "quantization": "awq",
            "gpu_memory_utilization": 0.90,
            "tensor_parallel_size": 4,
            "cache_dir": "models/text/cache/llama-70b",
            "checkpoint_dir": "models/text/checkpoints/llama-70b"
        }
    },
    "optimization": {
        "use_flash_attention": true,
        "use_triton": true,
        "use_cuda_graphs": true,
        "max_num_batched_tokens": 8192,
        "max_num_seqs": 256,
        "gpu_memory_utilization": 0.90,
        "max_model_len": 32768,
        "quantization": {
            "bits": 4,
            "group_size": 128,
            "zero_point": true,
            "double_quant": true
        }
    },
    "cache": {
        "type": "etcd",
        "ttl": 3600,
        "max_size": 10000,
        "embedding_cache": {
            "enabled": true,
            "max_size": 1000,
            "ttl": 7200
        }
    }
} 